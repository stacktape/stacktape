---
title: "Worker Services"
order: 120
---

<br />
<br />

A worker service is a continuously running container that is not directly accessible from outside your stack. It's ideal for background jobs, such as processing items from a message queue or handling other asynchronous tasks.

Key features include:

- **Automatic scaling:** Scales based on CPU or memory usage.
- **Zero-downtime deployments:** New versions are deployed without interrupting the service.
- **Flexible container images:** Supports various methods for providing a container image, including auto-packaging for popular languages.
- **Fully managed:** No need to manage servers, operating systems, or virtual machines.
- **Seamless connectivity:** Easily connects to other resources within your stack.

# How it works

Stacktape uses AWS Elastic Container Service (ECS) to run your containers on either _Fargate_ or _EC2 instances_.

- **_Fargate_** is a _serverless_ compute engine that runs containers without requiring you to manage the underlying servers.
- **_EC2 instances_** are virtual servers that give you more control over the computing environment.

ECS services are self-healing, automatically replacing any container that fails. They also scale automatically based on the rules you define.

# When to use it

This table helps you choose the right container-based resource for your needs:

| **Resource type**                                                         | **Description**                                                                      | **Use-cases**                                  |
| ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | ---------------------------------------------- |
| [web-service](/compute-resources/web-services/)                           | A container with a **public endpoint and URL**.                                      | Public APIs, websites                          |
| [private-service](/compute-resources/private-services/)                   | A container with a **private endpoint**, accessible only within your stack.          | Private APIs, internal services                |
| [worker-service](/compute-resources/worker-services/)                     | A container that runs continuously but is **not directly accessible**.               | Background processing, message queue consumers |
| [multi-container-workload](/compute-resources/multi-container-workloads/) | A customizable workload with multiple containers, where you define the accessibility of each one. | Complex, multi-component services              |
| [batch-job](/compute-resources/batch-jobs/)                               | A container that runs a single job and then terminates.                              | One-off or scheduled data processing tasks     |

## Advantages

- **Control over the environment:** Runs any Docker image or an image built from a Dockerfile.
- **Cost-effective for predictable loads:** Cheaper than Lambda functions for services with steady traffic.
- **Load-balanced and scalable:** Automatically scales horizontally based on CPU and memory usage.
- **Highly available:** Runs across multiple _Availability Zones_ to ensure resilience.
- **Secure by default:** The underlying environment is managed and secured by AWS.

## Disadvantages

- **Slower scaling:** Adding new container instances can take several seconds to a few minutes, which is slower than the nearly-instant scaling of Lambda functions.
- **Not fully serverless:** Cannot scale down to zero. You pay for at least one running instance (starting at ~$8/month), even if it's idle.

# Basic usage

Here's a basic example of a worker service configuration:

`embed:worker-services/basic-usage.stp.yml`

> Example worker service configuration.

And here's the corresponding application code:

`embed:worker-services/basic-usage.ts`

> Example worker container in TypeScript (`main.ts`).

<br />

<PropertiesTable definitionName="WorkerService" />

# Image

A worker service runs a Docker image. You can provide this image in four ways:

- **[stacktape-image-buildpack](/configuration/packaging#stacktape-image-buildpack-worker-service):** Automatically packages your code without needing a Dockerfile.
- **[external-buildpack](/configuration/packaging#external-buildpack-worker-service):** Uses external buildpacks to create an image.
- **[custom-dockerfile](/configuration/packaging#custom-dockerfile-worker-service):** Builds an image from your own Dockerfile.
- **[prebuilt-images](/configuration/packaging#prebuilt-image-worker-service):** Uses an existing image from a container registry.

# Environment variables

<PropDescription definitionName="LocalScriptProps" propertyName="environment" descType="ld" />

`embed:worker-services/env.stp.yml`

<PropertiesTable definitionName="EnvironmentVar" searchForReferencesInDefinition="WorkerServiceProps" />

<Divider />

# Health check

Health checks monitor your container to ensure it's running correctly. If a container fails its health check, it's automatically terminated and replaced with a new one.

<PropertiesTable definitionName="ContainerHealthCheck" searchForReferencesInDefinition="WorkerServiceProps" />

<br />

For example, this health check uses `curl` to send a request to the service every 20 seconds. If the request fails or takes longer than 5 seconds, the check is considered failed.

`embed:worker-services/healthcheck.stp.yml`

# Shutdown

When a service instance is shut down (for example, during a deployment or when the stack is deleted), all of its containers receive a `SIGTERM` signal. This gives your application a chance to shut down gracefully.

By default, the application has 2 seconds to clean up before it's forcefully stopped with a `SIGKILL` signal. You can change this with the `stopTimeout` property (from 2 to 120 seconds).

`embed:multi-container-workloads/shutdown.ts`

> Example of a cleanup function that runs before the container shuts down.

# Logging

Anything your application writes to `stdout` or `stderr` is captured and stored in AWS CloudWatch.

You can view logs in a few ways:

- **Stacktape Console:** Find a direct link to the logs in the [Stacktape Console](https://console.stacktape.com/).
- **Stacktape CLI:** Use the [`stacktape logs`](/cli/commands/logs) command to stream logs to your terminal.
- **AWS Console:** Browse logs directly in the AWS CloudWatch console. The `stacktape stack-info` command can provide a link.

Log storage can be expensive. To manage costs, you can configure `retentionDays` to automatically delete logs after a certain period.

<PropertiesTable
  definitionName="ContainerWorkloadContainerLogging"
  searchForReferencesInDefinition="WorkerServiceProps"
/>

## Forwarding logs

You can forward logs to third-party services. See [Forwarding Logs](/configuration/log-forwarding/) for more information.

<Divider />

# Compute resources

In the `resources` section, you configure the CPU, memory, and instance types for your service. You can run your containers using either _Fargate_ or _EC2 instances_.

- **_Fargate_** is a _serverless_ option that lets you run containers without managing servers. You only need to specify the `cpu` and `memory` your service requires. It's a good choice for applications that need to meet high security standards like PCI DSS Level 1 and SOC 2.
- **_EC2 instances_** are virtual servers that give you more control. You choose the instance types that best fit your needs, and ECS places your containers on them.

<Info>

Regardless of whether you use _Fargate_ or _EC2 instances_, your containers run securely within a _VPC_.

</Info>

<PropDescription definitionName="WorkerServiceProps" propertyName="resources" descType="ld" />

<PropertiesTable
  definitionName="ContainerWorkloadResourcesConfig"
  searchForReferencesInDefinition="WorkerServiceProps"
/>

## Using Fargate

To use _Fargate_, specify `cpu` and `memory` in the `resources` section without including `instanceTypes`.

`embed:worker-services/resources.stp.yml`

> Example of a service running on _Fargate_.

## Using EC2 instances

To use _EC2 instances_, specify a list of `instanceTypes` in the `resources` section.

<PropDescription definitionName="ContainerWorkloadResourcesConfig" propertyName="instanceTypes" descType="ld" />

`embed:worker-services/resources-ec2.stp.yml`

> Example of a service running on _EC2 instances_.

### Container placement on EC2

Stacktape tries to use your EC2 instances as efficiently as possible.

- If you specify `instanceTypes` without `cpu` and `memory`, Stacktape configures each service instance to use the full resources of one EC2 instance. When the service scales out, a new EC2 instance is added for each new service instance.
- If you specify `cpu` and `memory`, AWS will place multiple service instances on a single EC2 instance if there's enough capacity, maximizing utilization.

### Default CPU and memory for EC2

- If `cpu` is not specified, containers on an EC2 instance share its CPU capacity.
- If `memory` is not specified, Stacktape sets the memory to the maximum amount available on the smallest instance type in your `instanceTypes` list.

### Using a warm pool

A warm pool keeps pre-initialized EC2 instances in a stopped state, allowing your service to scale out much faster. This is useful for handling sudden traffic spikes. You only pay for the storage of stopped instances, not for compute time.

To enable it, set `enableWarmPool` to `true`. This feature is only available when you specify exactly one instance type.

For more details, see the [AWS Auto Scaling warm pools documentation](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html).

`embed:web-services/warm-pool.stp.yml`

# Scaling

The `scaling` section lets you control how your service scales. You can set the minimum and maximum number of running instances and define a policy that triggers scaling actions.

<PropertiesTable definitionName="ContainerWorkloadScaling" searchForReferencesInDefinition="WorkerServiceProps" />

## Scaling policy

A scaling policy defines the CPU and memory thresholds that trigger scaling.

- **Scaling out (adding instances):** The service scales out if either the average CPU or memory utilization exceeds the target you set.
- **Scaling in (removing instances):** The service scales in only when both CPU and memory utilization are below their target values.

The scaling process is more aggressive when adding capacity than when removing it. This helps ensure your application can handle sudden increases in load, while scaling in more cautiously to prevent flapping (scaling in and out too frequently).

<PropertiesTable definitionName="ContainerWorkloadScalingPolicy" searchForReferencesInDefinition="WorkerServiceProps" />

`embed:worker-services/scaling.stp.yml`

> Example of a scaling configuration.

# Storage

Each service instance has its own temporary, or _ephemeral storage_, with a fixed size of 20GB. This storage is deleted when the instance is removed. Different instances of the same service do not share their storage.

For persistent data storage, use [Buckets](/other-resources/buckets).

# Accessing other resources

By default, AWS resources cannot communicate with each other. Access must be granted using _IAM_ permissions.

Stacktape automatically configures the necessary permissions for the services it manages. For example, it allows a worker service to write logs to CloudWatch.

However, if your application needs to access other resources, you must grant permissions manually. You can do this in two ways:

## Using connectTo

The `connectTo` property lets you grant access to other Stacktape-managed resources by simply listing their names. Stacktape automatically configures the required _IAM_ permissions and injects connection details as environment variables into your service.

`embed:worker-services/allow-access-to.stp.yml`

<br />

<PropDescription definitionName="LambdaFunctionProps" propertyName="connectTo" descType="ld" />

## Using iamRoleStatements

For more granular control, you can provide a list of raw _IAM_ role statements. These statements are added to the service's _IAM_ role, allowing you to define precise permissions for any AWS resource.

`embed:worker-services/iam-role-statements.stp.yml`

<PropertiesTable definitionName="StpIamRoleStatement" searchForReferencesInDefinition="WorkerServiceProps" />

# Default VPC connection

Some AWS services, like relational databases, must be deployed within a _VPC_. If your stack includes such resources, Stacktape automatically creates a default _VPC_ and connects them to it.

Worker services are connected to this default _VPC_ by default, allowing them to communicate with other VPC-based resources without extra configuration.

To learn more, see the documentation on [_VPCs_](/user-guides/vpcs) and resource accessibility.

# Pricing

When using _Fargate_, you are charged for:

- **vCPU per hour:** ~$0.04 - $0.07, depending on the region.
- **Memory (GB) per hour:** ~$0.004 - $0.008, depending on the region.

Usage is billed by the second, with a one-minute minimum. For more details, see [AWS Fargate pricing](https://aws.amazon.com/fargate/pricing/).

# API reference

<PropertiesTable
  definitionName="ContainerLanguageSpecificConfig"
  searchForReferencesInDefinition="WorkerServiceProps"
/>

<PropertiesTable definitionName="StpIamRoleStatement" searchForReferencesInDefinition="WorkerServiceProps" />
